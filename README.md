# ğŸ—ï¸ Medallion Architecture Data Pipeline (FMCG Project) â€“ Databricks Lakehouse

An end-to-end **production-style Data Engineering project** implementing the Medallion Architecture (Bronze â†’ Silver â†’ Gold) using **Databricks, PySpark, Delta Lake, and AWS S3**.

This project demonstrates scalable distributed processing, transactional data lakes, dimensional modeling, and BI-ready data delivery.

---

---

## ğŸ“š Table of Contents

- [ğŸ“Œ Executive Summary](#-executive-summary)
- [ğŸ§  Architecture Overview](#-architecture-overview)
  - [ğŸ”¹ Bronze Layer (Raw Delta)](#-bronze-layer-raw-delta)
  - [ğŸ”¹ Silver Layer (Cleaned Delta)](#-silver-layer-cleaned-delta)
  - [ğŸ”¹ Gold Layer (Aggregated / BI Ready)](#-gold-layer-aggregated--bi-ready)
- [ğŸ› ï¸ Tech Stack](#-tech-stack)
- [ğŸ“‚ Repository Structure](#-repository-structure)
- [ğŸ“Š Dashboard Output](#-dashboard-output)
- [ğŸ”„ Data Flow](#-data-flow)
- [âš¡ Performance Optimization](#-performance-optimization)
- [ğŸ§ª Data Quality Checks](#-data-quality-checks)
- [ğŸš€ How to Run](#-how-to-run)
- [ğŸ“ˆ Scalability Considerations](#-scalability-considerations)
- [ğŸ”® Future Improvements](#-future-improvements)
- [ğŸ‘¨â€ğŸ’» Author](#-author)

---


## ğŸ“Œ Executive Summary

This pipeline ingests raw FMCG transactional data from AWS S3 into Databricks, processes it through structured Bronze, Silver, and Gold layers using Delta Lake, and delivers analytics-ready data to a business intelligence dashboard.

Key capabilities demonstrated:

- âœ… Lakehouse architecture implementation
- âœ… Delta Lake ACID transactions
- âœ… Schema enforcement & evolution
- âœ… Data quality validation
- âœ… Dimensional modeling (Star Schema)
- âœ… Query optimization using Z-Ordering
- âœ… Dashboard integration via Databricks SQL

---

## ğŸ§  Architecture Overview

![Architecture Diagram](docs/Architecture.png)

### ğŸ”¹ Bronze Layer (Raw Delta)
- Ingests raw CSV data from S3
- Stores as Delta tables
- Preserves historical data
- Adds metadata columns
- Supports CDC & reprocessing

### ğŸ”¹ Silver Layer (Cleaned Delta)
- Schema enforcement
- Deduplication
- Type casting
- Filtering invalid records
- Business logic transformations

### ğŸ”¹ Gold Layer (Aggregated / BI Ready)
- Fact & dimension tables
- Star schema modeling
- Aggregated KPIs
- Optimized for dashboard queries

---

## ğŸ› ï¸ Tech Stack

| Component        | Technology Used |
|-----------------|----------------|
| Compute         | Databricks (Spark 3.x) |
| Language        | PySpark (Python 3.x) |
| Storage         | Delta Lake |
| Cloud           | AWS S3 |
| Orchestration   | Databricks Workflows |
| BI              | Databricks SQL Dashboard |
| CI/CD           | GitHub Actions |

---

## ğŸ“‚ Repository Structure

```
databricks-medallion-lakehouse-fmcg/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ setup/
â”‚   â”‚   â”œâ”€â”€ setup_catalog.ipynb
â”‚   â”‚   â””â”€â”€ utilities.ipynb
â”‚   â”‚
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ 1_customers_data_processing.ipynb
â”‚   â”‚   â”œâ”€â”€ 2_products_data_processing.ipynb
â”‚   â”‚   â””â”€â”€ 3_pricing_data_processing.ipynb
â”‚   â”‚
â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”œâ”€â”€ dim_date_table_creation.ipynb
â”‚   â”‚   â”œâ”€â”€ 1_full_load_fact.ipynb
â”‚   â”‚   â””â”€â”€ 2_incremental_load_fact.ipynb
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Architecture.png
â”‚   â””â”€â”€ Dashboard.png
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

```

---

## ğŸ“Š Dashboard Output

![Sales Dashboard](docs/Dashboard.png)

### Key Business Metrics:
- Total Revenue: 14.63B
- Total Quantity: 5.2M
- Top 10 Products by Revenue
- Category-wise Sales Distribution

---

## ğŸ”„ Data Flow

### 1ï¸âƒ£ Bronze (Raw Ingestion)

```python
df = spark.read.format("csv") \
    .option("header", "true") \
    .load("s3://fmcg-landing-zone/transactions.csv")

df.write.format("delta") \
    .mode("append") \
    .save("/mnt/bronze/transactions")
```

---

### 2ï¸âƒ£ Silver (Data Cleaning & Validation)

```python
silver_df = bronze_df.dropDuplicates(["transaction_id"])

silver_df = silver_df.withColumn(
    "quantity", col("quantity").cast("int")
)

silver_df.write.format("delta") \
    .mode("overwrite") \
    .save("/mnt/silver/transactions")
```

---

### 3ï¸âƒ£ Gold (Business Modeling)

```python
fact_sales = silver_df.withColumn(
    "total_amount",
    col("quantity") * col("price")
)

fact_sales.write.format("delta") \
    .save("/mnt/gold/fact_sales")
```

---

## âš¡ Performance Optimization

### Partitioning
```python
.partitionBy("year")
```

### Z-Ordering
```sql
OPTIMIZE gold.fact_sales
ZORDER BY (product_id);
```

### Delta Time Travel
```sql
SELECT * FROM gold.fact_sales VERSION AS OF 2;
```

---

## ğŸ§ª Data Quality Checks

- Schema validation
- Null checks
- Deduplication enforcement
- Cross-field validation
- Transaction integrity

Example:

```python
assert df.filter(col("transaction_id").isNull()).count() == 0
```

---

## ğŸš€ How to Run

1. Upload raw data to AWS S3
2. Create Databricks cluster
3. Configure S3 access
4. Run:
   - bronze.py
   - silver.py
   - gold.py
5. Open Databricks SQL â†’ Create Dashboard

---

## ğŸ“ˆ Scalability Considerations

- Uses distributed Spark engine
- Minimizes shuffle via optimized joins
- Implements Delta compaction
- Supports large-scale datasets
- AQE (Adaptive Query Execution) compatible

---

## ğŸ”® Future Improvements

- Unity Catalog Integration
- Serverless Databricks
- Automated deployment via Terraform
- Streaming ingestion
- Great Expectations framework

---

## ğŸ‘¨â€ğŸ’» Author

Tanmay Kadlag  
Data Engineering | Lakehouse Architect | PySpark Enthusiast  

LinkedIn: https://www.linkedin.com/in/tanmay-kadlag25/

---

## â­ If you found this project useful, consider starring the repository.
